<!DOCTYPE html>
<html><head>
    <meta charset='utf-8'>
    <title>
      Jeff Zych
    </title>
    <meta content="Jeff Zych's website" name='description'>
    <meta content='Jeff Zych' name='author'>
    <meta content='width=device-width, initial-scale=1.0, user-scalable=yes' name='viewport'>
    <meta content='black' name='apple-mobile-web-app-status-bar-style'>
    <meta content='yes' name='apple-mobile-web-app-capable'>
    <!-- FB Open Graph meta tags -->
    <meta content='blog' property='og:type'>
    <meta content="Jeff Zych's Blog" property='og:site_name'>
    <meta content='http://jlzych.com/images/monogram.png' property='og:image'>
    <meta content='image/png' property='og:image:type'>
    <meta content='1024' property='og:image:width'>
    <meta content='1024' property='og:image:height'>
    <meta content='http://jlzych.com/' property='og:url'>
    <meta content="Jeff Zych's home page" property='og:title'>
    <link href="/favicon.ico?v=1" rel="icon" type="image/ico?v=1" />
    <link href="/stylesheets/application.css?1381713378" media="screen" rel="stylesheet" type="text/css" />
    <link href='/feed.xml' rel='alternate' title='RSS for Jeff Zych' type='application/rss+xml'>
    <script src='//use.typekit.net/kbr8kzk.js' type='text/javascript'></script>
    <script>
      try{Typekit.load();}catch(e){}
    </script>
  </head><body>
    <div id='main' role='main'>
      <header>
        <h1 class='monogram'>
                    <a title="Home" href="/"><img alt="JZ" src="/images/monogram.svg?1378005138" />
          Jeff Zych
          </a>

        </h1>
        <nav class='nav-main'>
          <ul>
            <li><a title="About Jeff Zych" href="/about">About</a></li>
            <li><a href="/archives">Archives</a></li>
          </ul>
        </nav>
      </header>
      <div class='content'>
        <article>
          <time datetime='2014-05-11'>
            <i>Posted on</i>
            May 11, 2014
            <i>by</i>
            Jeff Zych
          </time>
          <h1><a href="/2014/05/11/why-we-hire-ui-engineers-on-the-design-team/">Why We Hire UI Engineers on the Design Team</a></h1>
          <p>This <a href="http://cognition.happycog.com/article/why-developers-need-to-learn-design">Happy Cog article</a> by <a href="http://stephencaver.com/">Stephen Caver</a> perfectly encapsulates why we hire <a href="http://jobsco.re/1oHhZGz">UI Engineers</a> on the design team at <a href="https://www.optimizely.com">Optimizely</a> (as opposed to the engineering team). We want the folks coding a UI to be involved in the design process from the beginning, to understand the design system that underlies a user experience, and to be empowered to make design decisions while developing a UI. Successful designs must adapt to various contexts and degrade gracefully. The people most qualified to make those kinds of decisions are the ones writing the code. As said in the article, &ldquo;In this new world, the best thing a developer can do is to acquire an eye for design—to be able to take design aesthetic, realize its essential components, and reinterpret them on the fly.&rdquo; By embracing this mindset in our hiring and design process, we&rsquo;ve found the end result is a higher quality product.</p>
          <p class='follow-me'>
            Thoughts? Reply
            <a href="https://twitter.com/intent/tweet?screen_name=jlzych">@jlzych</a>
          </p>
          <footer class='tags'>
            Tags:
            <span class='tag'>design</span>
          </footer>
        </article>
        <article>
          <time datetime='2014-05-03'>
            <i>Posted on</i>
            May  3, 2014
            <i>by</i>
            Jeff Zych
          </time>
          <h1><a href="/2014/05/03/matthew-carters-my-life-in-typefaces/">Matthew Carter&#8217;s &ldquo;My Life in Typefaces&rdquo;</a></h1>
          <p>I just got around to watching Matthew Carter&rsquo;s excellent TED talk, <a href="http://on.ted.com/sqLe">&ldquo;My Life in Typefaces&rdquo;</a>. In it, he talks about his experience designing type for the past 5 decades, and how technical constraints influenced his designs. The central question he tries to answer is, &ldquo;Does a constraint force a compromise? By accepting a constraint, are you working to a lower standard?&rdquo; This is a question that comes up in every discipline, and with every technological change. Matthew Carter&rsquo;s take on this subject is interesting because he&rsquo;s experienced numerous technological changes, and has designed superb typefaces for all of them.</p>
          
          <p>At first blush, it&rsquo;s easy to conclude that constraints force designers to compromise their vision. But design isn&rsquo;t produced in a vacuum, and ultimately must be realized through one or more mediums (print, screen, radio, etc.). Therefore, one must work within constraints to produce the best designs. To do so, designers must understand the technology that enables their designs to be experienced, be it code, the printing process, and so on. As Matthew Carter said in this talk, &ldquo;I&rsquo;m a pragmatist, not an idealist, out of necessity,&rdquo; which is a valuable lesson that all designers should take to heart.</p>
          <p class='follow-me'>
            Thoughts? Reply
            <a href="https://twitter.com/intent/tweet?screen_name=jlzych">@jlzych</a>
          </p>
          <footer class='tags'>
            Tags:
          </footer>
        </article>
        <article>
          <time datetime='2014-04-16'>
            <i>Posted on</i>
            Apr 16, 2014
            <i>by</i>
            Jeff Zych
          </time>
          <h1><a href="/2014/04/16/designing-with-instinct-vs-data/">Designing with instinct vs. data</a></h1>
          <p><a href="http://www.gv.com/team/braden-kowitz">Braden Kowitz</a> wrote a <a href="http://www.gv.com/lib/design-instinct-vs-data">great article</a> exploring the ever increasing tension between making design decisions based on instinct versus data. As he says, &ldquo;It’s common to think of data and instincts as being opposing forces in design decisions.&rdquo; This is especially true at an <a href="https://www.optimizely.com">A/B testing company</a> — we have a tendency to quantify and measure everything.</p>
          
          <p>He goes on to say, &ldquo;In reality, there’s a blurry line between the two,&rdquo; and I couldn&rsquo;t agree more. When I&rsquo;m designing, I&rsquo;m in the habit of always asking, &ldquo;What data would help me make this decision?&rdquo; Sometimes it&rsquo;s usage logs, sometimes it&rsquo;s user testing, sometimes it&rsquo;s market research, and sometimes there isn&rsquo;t anything but my own intuition. Even when there is data, it&rsquo;s all just input I use to help me reach a decision. It&rsquo;s not a good idea to blindly follow data, but it&rsquo;s equally bad to only use your gut. As Braden said, it&rsquo;s important to balance the two.</p>
          <p class='follow-me'>
            Thoughts? Reply
            <a href="https://twitter.com/intent/tweet?screen_name=jlzych">@jlzych</a>
          </p>
          <footer class='tags'>
            Tags:
            <span class='tag'>product design</span>
          </footer>
        </article>
        <article>
          <time datetime='2014-03-11'>
            <i>Posted on</i>
            Mar 11, 2014
            <i>by</i>
            Jeff Zych
          </time>
          <h1><a href="/2014/03/11/did-you-a-b-test-the-redesigned-preview-tool/">Did you A/B test the redesigned preview tool?</a></h1>
          <p>A lot of people have asked me if we <a href="https://www.optimizely.com/ab-testing">A/B tested</a> the <a href="/2014/02/11/re-designing-optimizely-s-preview-tool/">redesigned preview tool</a>. The question comes in two flavors: did we use A/B testing to validate impersonation was worth building (a.k.a. fake door testing); and, did we A/B test the redesigned UI against the old UI? Both are good questions, but the short answer is no. In this post I&rsquo;m going to dig into both and explain why.</p>
          
          <h2>Fake Door Testing</h2>
          
          <p><a href="http://www.slideshare.net/JessLee4/fake-doors-how-to-test-product-ideas-quickly-hustlecon-2013">Fake door testing</a> (<a href="http://vimeo.com/24744647">video</a>) is a technique to measure interest in a new feature by building a &ldquo;fake door&rdquo; version of it that looks like the final version (but doesn&rsquo;t actually work) and measuring how many people engage with it. Trying to use the feature gives users an explanation of what it is, that it&rsquo;s &ldquo;Coming soon&rdquo;, and usually the ability to &ldquo;vote&rdquo; on it or send feedback (the specifics vary depending on the context). This doesn&rsquo;t need to be run as an A/B test, but setting it up as one lets you compare user behavior.</p>
          
          <p>We could have added an &ldquo;Impersonate&rdquo; tab to the old UI, measured how many people tried to use it, and gathered feedback. This would have been cheap and easy. But we didn&rsquo;t do this because the feature was inspired by our broader research around personalization. Our data pointed us towards this feature, so we were confident it would be worthwhile.</p>
          
          <p>But more than that, measuring clicks and votes doesn&rsquo;t tell you much. People can click out of curiosity, which doesn&rsquo;t tell you if they&rsquo;d actually use the feature or if it solves a real problem. Even if people send feedback saying they&rsquo;d love it, <a href="http://www.nngroup.com/articles/first-rule-of-usability-dont-listen-to-users/">what users say and what they do is different</a>. Actually talking to users to find pain points yields robust data that leads to richer solutions. The new impersonate functionality is one such example — no one had thought of it before, and it wasn&rsquo;t on our feature request list or product roadmap.</p>
          
          <p>However, not everyone has the resources to conduct user research. In that situation, fake door testing is a good way of cheaply getting feedback on a specific idea. After all, some data is better than no data.</p>
          
          <h2>A/B Testing the Redesigned UI</h2>
          
          <p>The second question is, &ldquo;Did you A/B test the redesigned preview against the previous one?&rdquo; We didn&rsquo;t, primarily because there&rsquo;s no good metric to use as a conversion goal. We added completely new functionality to the preview tool, so most metrics are the equivalent of comparing apples to oranges. For example, measuring how many people impersonate a visitor is meaningless because the old UI doesn&rsquo;t even have that feature.</p>
          
          <p>So at an interface level, there isn&rsquo;t a good measurement. But what about at the product level? We could measure larger metrics, such as the number of targeted experiments being created, to see if the new UI has an effect. There are two problems with this. First, it will take a long time (many months) to reach a statistically significant difference because the conversion rate on most product metrics are low. Second, if we eventually measured a difference, there&rsquo;s no guarantee it was caused by adding impersonation. The broader the metric, the more factors that influence it (such as other product updates). To overcome this you could freeze people in the &ldquo;A&rdquo; and &ldquo;B&rdquo; versions of the product, but given how long it takes to reach significance, this isn&rsquo;t a good idea.</p>
          
          <p>Companies like Facebook and Google have enough traffic that they actually are able to roll out new features to a small percentage of users (say, 5%), and measure the impact on their core metrics. If any take a plunge, they revert users to the previous UI and keep iterating. When you have the scale of Facebook and Google, you can get significant data in a day. Unfortunately, like most companies, we don&rsquo;t have this scale, so it isn&rsquo;t an option.</p>
          
          <h2>So How Do You Know The Redesign Was Worth It?</h2>
          
          <p>What people are really asking is how do we know the redesign was worth the effort? Being at an A/B testing company, everyone wants to A/B test everything. But in this case, there wasn&rsquo;t a place for it. Like any method, <a href="http://www.nngroup.com/articles/ab-testing-usability-engineering/">split testing has its strengths and weaknesses</a>.</p>
          
          <h2>No, Really — How <em>Do</em> You Know The Redesign Was Worth It?</h2>
          
          <p>Primarily via qualitative feedback (i.e. talking to users), which at a high level has been positive (but there are some improvements we can make). We&rsquo;re also measuring people&rsquo;s activities in the preview tool (e.g. changing tabs, impersonating visitors, etc.). So far, those are healthy. Finally, we&rsquo;re keeping an eye on some product-level metrics, like the number of targeted experiments created. These metrics are part of our long-term personalization efforts, and we hope in the long run to see them go up. But the preview tool is just one piece of that puzzle, so we don&rsquo;t expect anything noticeable from that alone.</p>
          
          <p>The important theme here is that gathering data is important to ensure you&rsquo;re making the best use of limited resources (time, people, etc.). But there&rsquo;s a whole world of data beyond A/B testing, such as user research, surveys, product analytics, and so on. It&rsquo;s important to keep in mind the advantages and disadvantages of each, and use the most appropriate ones at your disposal.</p>
          <p class='follow-me'>
            Thoughts? Reply
            <a href="https://twitter.com/intent/tweet?screen_name=jlzych">@jlzych</a>
          </p>
          <footer class='tags'>
            Tags:
            <span class='tag'>a/b testing</span>
            <span class='tag'>Optimizely</span>
          </footer>
        </article>
        <article>
          <time datetime='2014-03-02'>
            <i>Posted on</i>
            Mar  2, 2014
            <i>by</i>
            Jeff Zych
          </time>
          <h1><a href="/2014/03/02/when-we-do-user-testing/">When Do You Do User Testing?</a></h1>
          <p>In response to my <a href="/2014/02/11/re-designing-optimizely-s-preview-tool/">preview redesign post</a>, my uncle asked, &ldquo;When does the designer go with their own analysis of a design and when do they do a usability test?&rdquo; I&rsquo;ve been asked this question a lot, and we often discuss it in product development meetings. It&rsquo;s also something I wanted to elaborate on more in the post, but it didn&rsquo;t fit. So I will take this opportunity to roughly outline when we do user testing and why.</p>
          
          <h2>The Ideal</h2>
          
          <p>In an ideal world, we would be testing all the time. Having a tight feedback loop is invaluable. By which I mean, being able to design a solution to a problem and validating that it solves said problem immediately would be amazing. And in product design, the best validation is almost always user testing.</p>
          
          <h2>The Reality</h2>
          
          <p>In reality, there&rsquo;s no such thing as instant feedback. You can&rsquo;t design something and receive immediate feedback. There&rsquo;s no automated process that tells you if a UI is good or not. You have to talk to actual humans, which takes time and effort.</p>
          
          <p>This means there&rsquo;s a trade-off between getting as much feedback as possible, and actually releasing something. The more user testing you do, the longer it will take to release.</p>
          
          <h3>What we do at Optimizely</h3>
          
          <p>At <a href="https://www.optimizely.com">Optimizely</a>, we weigh the decision to do user testing against deadlines, how important the questions are (e.g. are they core to the experience, or an edge case), how likely we are to get actionable insights (e.g. testing colors will usually give you a bunch of conflicting opinions), and what other research we could be doing instead (i.e. opportunity cost).</p>
          
          <p>With the preview redesign, we started with exploratory research to get us on the right track, but didn&rsquo;t do much testing beyond that. This was mainly because we didn&rsquo;t make time for it. It was clear from our generative research that adding impersonation to the preview tool would be a step in the right direction. But it&rsquo;s only one part of a larger solution, and won&rsquo;t be used by everyone. We didn&rsquo;t want to slow down our overall progress by spending too much time trying to perfect one piece of a much larger puzzle.</p>
          
          <p>So with the preview tool, I had to rely on my instincts and feedback from other designers, engineers, and product managers to make decisions. One such example is when I decided to hide the impersonation feature by default, and it would slide out when an icon is clicked. Of this solution I said:</p>
          
          <blockquote>
          <p>But it worked <em>too</em> well [at solving the problem of the impersonation UI being distracting]. The icon was too cryptic, and overall the impersonate functionality was too hidden for anyone to find.</p>
          </blockquote>
          
          <p>As my uncle pointed out, I didn&rsquo;t do any user testing to make this call. I looked at what I had created, and could tell it wasn&rsquo;t a great solution (which was also confirmed by my teammates). I was confident the decision to keep iterating was right based on established <a href="http://www.nngroup.com/articles/ten-usability-heuristics/">usability heuristics</a> and my own experience.</p>
          
          <p>However, not all design decisions can be guided by general usability guidelines. One such example is when I went in circles designing how a person sets impersonation values. I tried using established best practices and talking to other designers, but neither method led me to an answer. At this point, user testing was my only out.</p>
          
          <p>In this case, we opted for <a href="http://www.nngroup.com/articles/guerrilla-hci/">guerrilla usability testing</a>. Recruiting actual users would have required more time and resources than we wanted to spend. So I called in two of our sales guys, who made for good proxies of semi-experienced users that are technically middle-of-the-road (i.e. they have some basic code knowledge, but aren&rsquo;t developers), which covers the majority of our users. Their feedback made the decision easy, and successfully got me out of this jam.</p>
          
          <h2>In Summary</h2>
          
          <p>In a perfect world we would be testing all the time, but in reality that just isn&rsquo;t feasible. So we do our best to balance the time and resources required to test a UI against the overall importance of that feature. But usability testing won&rsquo;t find every flaw. Eventually you have to ship — you can&rsquo;t refine forever, and no design is perfect.</p>
          <p class='follow-me'>
            Thoughts? Reply
            <a href="https://twitter.com/intent/tweet?screen_name=jlzych">@jlzych</a>
          </p>
          <footer class='tags'>
            Tags:
            <span class='tag'>Optimizely</span>
          </footer>
        </article>
        <a class="view-all-link" href="/archives">View all posts &raquo;</a>
      </div>
      <footer id='site-footer'>
        <p>
          &copy; 2010&ndash;2014 Jeff Zych
          &bull; Hand-crafted with
          <a href="http://middlemanapp.com/">Middleman</a>
        </p>
        <a style="display: none;" href="http://plus.google.com/+JeffZych?rel=author">Google+ profile</a>
      </footer>
    </div>
    <!-- Google Analytics tracking -->
    <script>
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-21389659-1']);
      _gaq.push(['_trackPageview']);
      
      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
  </body>
</html>
